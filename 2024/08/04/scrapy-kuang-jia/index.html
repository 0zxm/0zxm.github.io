<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>scrapy框架 | 0zxm</title><meta name="author" content="0zxm"><meta name="copyright" content="0zxm"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="python爬虫scrapy框架">
<meta property="og:type" content="article">
<meta property="og:title" content="scrapy框架">
<meta property="og:url" content="https://0zxm.github.io/2024/08/04/scrapy-kuang-jia/index.html">
<meta property="og:site_name" content="0zxm">
<meta property="og:description" content="python爬虫scrapy框架">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://0zxm.github.io/img/cover1.jpg">
<meta property="article:published_time" content="2024-08-04T03:44:37.000Z">
<meta property="article:modified_time" content="2025-01-12T10:37:27.737Z">
<meta property="article:author" content="0zxm">
<meta property="article:tag" content="python">
<meta property="article:tag" content="scrapy">
<meta property="article:tag" content="数据持久化存储">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://0zxm.github.io/img/cover1.jpg"><link rel="shortcut icon" href="/favicon.png"><link rel="canonical" href="https://0zxm.github.io/2024/08/04/scrapy-kuang-jia/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: 0zxm","link":"链接: ","source":"来源: 0zxm","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'scrapy框架',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-12 18:37:27'
}</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">65</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group hide"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/src/"><i class="fa-fw fas fa-cloud"></i><span> 资源</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/cover1.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">0zxm</span></a><a class="nav-page-title" href="/"><span class="site-name">scrapy框架</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group hide"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/src/"><i class="fa-fw fas fa-cloud"></i><span> 资源</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">scrapy框架</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-04T03:44:37.000Z" title="发表于 2024-08-04 11:44:37">2024-08-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-12T10:37:27.737Z" title="更新于 2025-01-12 18:37:27">2025-01-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">9.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>39分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Scrapy框架简介"><a href="#Scrapy框架简介" class="headerlink" title="Scrapy框架简介"></a>Scrapy框架简介</h2><p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/">Scrapy官方文档</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1jx411b7E3/">B站黑马教程</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Db4y1m7Ho/">B站尚硅谷教程</a></p>
<p><a href="http://0zxm.github.io/PDFs/%E7%88%AC%E8%99%AB%E4%BB%A3%E7%A0%81.zip">代码文件</a></p>
<h3 id="scrapy框架"><a href="#scrapy框架" class="headerlink" title="scrapy框架"></a>scrapy框架</h3><ul>
<li>Scrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用途非常广泛。</li>
<li>框架的力量，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。</li>
<li>Scrapy 使用了 <code>Twisted[&#39;twrstrd]</code>(其主要对手是Tornado)异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求</li>
</ul>
<h3 id="scrapy是什么？"><a href="#scrapy是什么？" class="headerlink" title="scrapy是什么？"></a>scrapy是什么？</h3><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理<br>或存储历史数据等一系列的程序中。</p>
<h3 id="安装scrapy"><a href="#安装scrapy" class="headerlink" title="安装scrapy"></a>安装scrapy</h3><pre class="line-numbers language-cmd" data-language="cmd"><code class="language-cmd">安装非Python的依赖 
sudo apt-get install python-dev python-pip libxm12-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev(Ubuntu下安装)

pip install scrapy <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>安装过程中出错：</p>
<ol>
<li><p>如果安装有错误！！！！</p>
 <pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">pip install Scrapy
building 'twisted.test.raiser' extension
error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft Visual C++ 
Build Tools": http://landinghub.visualstudio.com/visual‐cpp‐build‐tools<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
</ol>
<p>解决方案：<br><a target="_blank" rel="noopener" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a>         </p>
<p>下载twisted对应版本的<code>whl</code>文件（如我的Twisted‐17.5.0‐cp36‐cp36m‐win_amd64.whl），cp后面是python版本，amd64代表64位，运行命令：<br><code>pip install C:\Users\...\Twisted‐17.5.0‐cp36‐cp36m‐win_amd64.whl   </code>     </p>
<p><code>pip install Scrapy  </code>      </p>
<ol start="2">
<li>如果再报错: <code>python ‐m pip install ‐‐upgrade pip     </code>       </li>
<li>如果再报错  win32</li>
</ol>
<p>解决方法：<br>   <code>pip install pypiwin32    </code>  </p>
<ol start="4">
<li>再报错：使用anaconda<br> 使用步骤：        <ul>
<li>打开anaconda           </li>
<li>点击environments            </li>
<li>点击not installed             </li>
<li>输入scrapy             </li>
<li>apply(应用)             </li>
<li>在<strong>pycharm</strong>中选择anaconda的环境</li>
</ul>
</li>
</ol>
<h2 id="Scrapy架构图-绿线是数据流向"><a href="#Scrapy架构图-绿线是数据流向" class="headerlink" title="Scrapy架构图(绿线是数据流向)"></a>Scrapy架构图(绿线是数据流向)</h2><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/scrapy架构图.png" alt="image" style="zoom: 67%;">

<ul>
<li><code>Scrapy Engine</code>(引擎):负责<code>Spider</code>、<code>Item Pipeline</code>、<code>Downloader</code>、<code>Scheduler</code> 中间的通讯，信号、数据传递等。</li>
<li><code>Scheduler</code>(调度器):它负责接受<strong>引擎</strong>发送过来的Request请求，并按照一定的方式进行整理排列，入队，当<strong>引擎</strong> 需要时，交还给<strong>引擎</strong>。</li>
<li><code>Downloader</code>(下载器):负责下载 <code>Scrapy Engine</code>(引擎)发送的所有Requests请求，并将其获取到的Responses交还给 <code>Scrapy Engine</code>(引擎)，由引擎交给 <code>Spider</code> 来处理。</li>
<li><code>Item Pipeline</code>(管道):它负责处理 <code>Spider</code> 中获取到的 Item(有用的数据，需要存储的)，并进行进行后期处理(详细分析、过滤、存储等)的地方.</li>
<li><code>Downloader middlewares</code>(下载中间件):你可以当作是一个可以自定义扩展下载功能的组件。</li>
<li><code>Spider Middlewares</code>(spider中间件):你可以理解为是一个可以自定扩展和操作<code>引擎</code>和<code>Spider</code>中间<strong>通信</strong>的功能组件(比如进入<code>Spider</code>的<strong>Responses</strong>;和从<code>Spider</code>出去的<strong>Requests</strong>)</li>
</ul>
<h3 id="scrapy架构流程"><a href="#scrapy架构流程" class="headerlink" title="scrapy架构流程"></a>scrapy架构流程</h3><ul>
<li><p>（1）<strong>引擎</strong>                       ‐‐‐》自动运行，无需关注，会自动组织所有的请求对象，分发给下载器              </p>
</li>
<li><p>（2）<strong>下载器</strong>    	         ‐‐‐》从引擎处获取到请求对象后，请求数据                     </p>
</li>
<li><p>（3）<strong>Spiders</strong> 	         ‐‐‐》Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</p>
</li>
<li><p>（4）<strong>调度器</strong>    	          ‐‐‐》有自己的调度规则，无需关注                     </p>
</li>
<li><p>（5）<strong>管道（Item Pipeline）</strong>       ‐‐‐》最终处理数据的管道，会预留接口供我们处理数据.当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。每个itempipeline组件(有时称之为<strong>“Item Pipeline”</strong>)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。</p>
</li>
<li><p>以下是item pipeline的一些典型应用：</p>
<ol>
<li><p>清理HTML数据</p>
</li>
<li><p>验证爬取的数据(检查item包含某些字段)</p>
</li>
<li><p>查重(并丢弃)</p>
</li>
<li><p>将爬取结果保存到数据库中</p>
</li>
</ol>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/scrapy%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt="image-20240802222000457"></p>
<h2 id="Scrapy项目的运行流程"><a href="#Scrapy项目的运行流程" class="headerlink" title="Scrapy项目的运行流程"></a>Scrapy项目的运行流程</h2><p>代码写好，程序开始运行 </p>
<ol>
<li>引擎: Hi!Spider,你要处理哪一个网站?</li>
<li>Spider: 老大要我处理xxxx.com。</li>
<li>引擎: 你把第一个需要处理的URL给我吧。</li>
<li>Spider: 给你，第一个URL是 xxxxxxx.com。</li>
<li>引擎: Hi!调度器，我这有request请求,你帮我排序入队一下</li>
<li>调度器: 好的，正在处理,你等一下。</li>
<li>引擎: Hi!调度器，把你处理好的request请求给我。</li>
<li>调度器: 给你，这是我处理好的request</li>
<li>引擎: Hi!下载器，你按照老大的<code>下载中间件</code>的设置帮我下载一下这个request请求</li>
<li>下载器: 好的!给你，这是下载好的东西。(如果失败:sorry，这个request下载失败了。然后<code>引擎</code>告诉<code>调度器</code>，这个request下载失败了，你记录一下，我们待会儿再下载)</li>
<li>引擎: Hi! spider，这是下载好的东西，并且已经按照老大的<code>下载中间件</code>处理过了，你自己处理一下(注意!这儿responses默认是交给 <strong>def parse()</strong> 这个函数处理的)</li>
<li>Spider: (处理完毕数据之后对于需要跟进的URL)，Hi!引擎，我这里有两个结果，这个是我需要<br>跟进的URL，还有这个是我获取到的Item数据。</li>
<li>引擎: Hi !管道 我这儿有个Item你帮我处理一下!调度器!这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。</li>
<li>管道、调度器: 好的，现在就做!</li>
</ol>
<p><strong>注意!只有当<code>调度器</code>中不存在任何request了,整个程序才会停止，(也就是说,对于下载失败的URL,Scrapy也会重新下载。)</strong></p>
<h2 id="制作Scrapy爬虫一共四步"><a href="#制作Scrapy爬虫一共四步" class="headerlink" title="制作Scrapy爬虫一共四步"></a>制作Scrapy爬虫一共四步</h2><ul>
<li><p>新建项目(scrapy start project xxx):新建一个新的爬虫项词。</p>
</li>
<li><p>明确目标(编写items.py):明确你想要抓取的目标。</p>
</li>
<li><p>制作爬虫(spiders&#x2F;xxspider.py):制作爬虫开始爬取网页。</p>
</li>
<li><p>存储内容(pipelines.py):设计管道存储爬取内容。</p>
</li>
</ul>
<h3 id="1-创建scrapy项目："><a href="#1-创建scrapy项目：" class="headerlink" title="1.创建scrapy项目："></a>1.创建scrapy项目：</h3><pre><code>  终端输入  `scrapy startproject  项目名称`    	 
</code></pre>
<h3 id="2-项目组成："><a href="#2-项目组成：" class="headerlink" title="2.项目组成："></a>2.项目组成：</h3><ul>
<li>spiders <ul>
<li>__init__.py</li>
<li>自定义的爬虫文件.py        ‐‐‐》由我们自己创建，是实现爬虫核心功能的文件</li>
</ul>
</li>
<li>_<em>init</em>_.py                  </li>
<li>items.py             ‐‐‐》定义数据结构的地方，是一个继承自scrapy.Item的类</li>
<li>middlewares.py       ‐‐‐》中间件   代理</li>
<li>pipelines.py         ‐‐‐》管道文件，里面只有一个类，用于处理下载数据的后续处理,默认是300优先级，值越小优先级越高（1‐1000）                                      </li>
<li>settings.py          ‐‐‐》配置文件  比如：是否遵守robots协议，User‐Agent					    定义</li>
</ul>
<h3 id="3-创建爬虫文件："><a href="#3-创建爬虫文件：" class="headerlink" title="3.创建爬虫文件："></a>3.创建爬虫文件：</h3><ul>
<li><p>（1）跳转到spiders文件夹   cd 目录名字&#x2F;目录名字&#x2F;spiders           	</p>
</li>
<li><p>（2）scrapy genspider 爬虫名字 网页的域名             </p>
</li>
<li><p>爬虫文件的基本组成：</p>
<ul>
<li>继承scrapy.Spider类                                		<ol>
<li>name &#x3D; ‘baidu’        ‐‐‐》 运行爬虫文件时使用的名字</li>
<li>allowed_domains       ‐‐‐》爬虫允许的域名，在爬取的时候，如果不是此域名之下的url，会被过滤掉</li>
<li>start_urls            ‐‐‐》 声明了爬虫的起始地址，可以写多个url，一般是一个                        </li>
<li>parse(self, response) ‐‐‐》解析数据的回调函数<ol>
<li>response.text         ‐‐‐》响应的是字符串</li>
<li>response.body         ‐‐‐》响应的是二进制文件</li>
<li>response.xpath()      –‐》xpath方法的返回值类型是selector列表</li>
<li>extract()             ‐‐‐》提取的是selector对象的是data</li>
<li>extract_first()       ‐‐‐》提取的是selector列表中的第一个数据</li>
</ol>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="4-运行爬虫文件"><a href="#4-运行爬虫文件" class="headerlink" title="4.运行爬虫文件:"></a>4.运行爬虫文件:</h3><pre><code>`scrapy crawl 爬虫名称`

注意:应在spiders文件夹内执行
</code></pre>
<h2 id="Scrapy框架终端基本命令"><a href="#Scrapy框架终端基本命令" class="headerlink" title="Scrapy框架终端基本命令"></a>Scrapy框架终端基本命令</h2><ul>
<li>scrapy bench 测试性能(pages&#x2F;min)</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/scrapy_fetch.png" alt="scrapy_fetch"></p>
<ul>
<li>scrapy fetch ‘<a target="_blank" rel="noopener" href="http://www.baidu.com’/">http://www.baidu.com’</a> 爬取百度页面的源代码,DEBUG信息(200)表示爬虫程序正常运行</li>
<li>genspider 创建爬虫</li>
<li>runspider 启动爬虫</li>
<li>shell 使用scrapy的shell环境</li>
<li>startproject 创建项目</li>
<li>version 显示版本</li>
<li>view 使用浏览器视图</li>
<li>list 显示当前项目有多少个爬虫程序</li>
</ul>
<h2 id="入门案例"><a href="#入门案例" class="headerlink" title="入门案例"></a>入门案例</h2><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul>
<li>创建一个Scrapy项目</li>
<li>定义提取的结构化数据(Item)</li>
<li>编写爬取网站的 Spider 并提取出结构化数据(Item)</li>
<li>编写 Item Pipelines 来存储提取到的Item(即结构化数据)</li>
</ul>
<h3 id="1-新建爬虫项目"><a href="#1-新建爬虫项目" class="headerlink" title="1.新建爬虫项目"></a>1.新建爬虫项目</h3><p>在开始爬取之前，必须创建一个新的Scrapy项目。进入自定义的项目目录中，运行下列命令<br><code>scrapy startproject mySpider</code></p>
<h3 id="2-新建爬虫"><a href="#2-新建爬虫" class="headerlink" title="2.新建爬虫"></a>2.新建爬虫</h3><p>跳转到mySpider&#x2F;mySpider&#x2F;spiders文件夹下</p>
<p><code>scrapy genspider 爬虫名 网页域名</code></p>
<p>网页域名的作用,让爬虫程序只在此域名下爬取</p>
<p>会在spiders文件夹下生成<code>爬虫名.py文件</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">ItcastSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>	<span class="token comment">#Spider类</span>
    name <span class="token operator">=</span> <span class="token string">"itcast"</span>	<span class="token comment"># 爬虫名称</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"www.itcast.cn"</span><span class="token punctuation">]</span>	<span class="token comment"># 爬虫运行的域名</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"http://www.itcast.cn"</span><span class="token punctuation">]</span>	<span class="token comment"># 起始url,爬虫程序启动的第一次请求目的url</span>

    <span class="token triple-quoted-string string">"""处理响应数据的方法"""</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>body<span class="token punctuation">)</span>
        <span class="token comment"># pass</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>后面还会学习到<code>CrawlSpider</code>类</p>
<h3 id="3-运行和检测爬虫程序"><a href="#3-运行和检测爬虫程序" class="headerlink" title="3.运行和检测爬虫程序"></a>3.运行和检测爬虫程序</h3><ul>
<li><strong>先看项目能不能正常运行在修改代码</strong></li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/crawl.png" alt="image-20240802130119274"></p>
<p>此时,输入scrapy会多一些可用指令</p>
<ul>
<li>scrapy check 爬虫名	检查爬虫是否正常</li>
<li>scrapy crawl 爬虫名        启动爬虫程序</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84.png" alt="image-20240802132150074"></p>
<p>下面来简单介绍一下各个主要文件的作用:</p>
<ul>
<li>scrapy.cfg :项目的配置文件</li>
<li>Itcast&#x2F;:项目的Python模块，将会从这里引用代码</li>
<li>Itcast&#x2F;items.py:项目的目标文件</li>
<li>Itcast&#x2F;pipelines.py:项目的管道文件</li>
<li>Itcast&#x2F;settings.py:项目的设置文件</li>
<li>Itcast&#x2F;spiders&#x2F;:存储爬虫代码目录</li>
</ul>
<h3 id="4-明确目标-Itcast-items-py"><a href="#4-明确目标-Itcast-items-py" class="headerlink" title="4.明确目标(Itcast&#x2F;items.py)"></a>4.明确目标(Itcast&#x2F;items.py)</h3><p>我们打算抓取:<a target="_blank" rel="noopener" href="http://www.itcast.cn/channel/teacher.shtml">http://www.itcast.cn/channel/teacher.shtml</a> 网站里的所有讲师的姓名、职称和个人信息。</p>
<ol>
<li>打开mySpider目录下的items.py,</li>
<li>Item 定义结构化数据字段用来保存爬取到的数据，有点像Python中的dict，但是提供了一些额外的保护减少错误。</li>
<li>可以通过创建一个 <code>scrapy.Item</code> 类，并且定义类型为 <code>scrapy.Field</code> 的类属性来定义一个 Item (可以理解成类似于ORM的映射关系)。</li>
<li>接下来，创建一个 <code>ItcastItem</code>类，和构建item模型(model)。</li>
</ol>
<h3 id="5-制作爬虫"><a href="#5-制作爬虫" class="headerlink" title="5.制作爬虫"></a>5.制作爬虫</h3><h4 id="爬数据-Itcast-spiders-itcast-py"><a href="#爬数据-Itcast-spiders-itcast-py" class="headerlink" title="爬数据(Itcast&#x2F;spiders&#x2F;itcast.py)"></a>爬数据(Itcast&#x2F;spiders&#x2F;itcast.py)</h4><p>在当前目录下输入命令，将在 myspider&#x2F;spider 目录下创建一个名为 itcast 的爬虫，并指定爬取域的范围:<code>Scrapy genspider itcast &quot;itcast.cn&quot;</code></p>
<p>打开 <code>Itcast/spider</code> 目录里的 itcast.py，默认增加了下列代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy

<span class="token keyword">class</span> <span class="token class-name">ItcastSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>	
    name <span class="token operator">=</span> <span class="token string">"itcast"</span>	
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"www.itcast.cn"</span><span class="token punctuation">]</span>	
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"http://www.itcast.cn"</span><span class="token punctuation">]</span>	

    <span class="token triple-quoted-string string">"""处理响应数据的方法"""</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># pass</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>其实也可以由我们自行创建 <strong>itcast.py</strong> 并编写上面的代码，只不过使用命令可以免去编写固定代码的麻烦</p>
<p>要建立一个Spider，你必须用scrapy.Spider类创建一个子类，并确定了<strong>三个强制的属性</strong>和<strong>一个方法</strong>。</p>
<ul>
<li><p><code>name = “”</code>: 这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</p>
</li>
<li><p><code>allow_domains = []</code>是搜索的域名范围，也就是爬虫的约束区域,规定爬虫只爬取这个域名下的网<br>  页，不存在的URL会被忽略。</p>
</li>
<li><p><code>start_urls = ()</code>:爬取的URL元祖&#x2F;列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些URL开始。其他子URL将会从这些起始URL中继承性生成。</p>
</li>
<li><p><code>parse(self，response)</code>:解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下:</p>
<ol>
<li>负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</li>
<li>生成需要下一页的URL请求。</li>
</ol>
</li>
</ul>
<p>将start_urls的值修改为需要爬取的第一个url</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">start_urls <span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">"http://ww.itcast.cn/channel/teacher.shtml"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>修改parse()方法</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"teacher.html"</span><span class="token punctuation">,</span><span class="token string">"w"</span><span class="token punctuation">)</span><span class="token keyword">as</span> f<span class="token punctuation">:</span>
        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>然后运行一下看看，在Itcast目录下执行</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scrapy crawl itcast<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>是的，就是 itcast,看上面代码，它是 <strong>ItcastSpider</strong> 类的 name 属性,也就是使用 <strong>scrapy genspider</strong> 命令的爬虫名。</p>
<p>一个Scrapy爬虫项目里，可以存在多个爬虫。各个爬虫在执行时，就是按照 name 属性来区分。</p>
<p>运行之后，如果打印的日志出现<code>[scrapy]INFO:Spider closed(finished)</code>，代表执行完成。之后当前文件夹中就出现了一个 teacher.html 文件，里面就是我们刚刚要爬取的网页的全部源代码信息</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 注意，Python2.x默认编码环境是ASCII码,当和取回的数据编码格式不一致时，可能会造成乱码;</span>
<span class="token comment"># 我们可以指定保存内容的编码格式，一般情况下，我们可以在代码最上方添加:</span>
<span class="token keyword">import</span> sys
<span class="token builtin">reload</span><span class="token punctuation">(</span>sys<span class="token punctuation">)</span>
sys<span class="token punctuation">.</span>setdefaultencoding<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span>
<span class="token comment">#这三行代码是Python2.x里解决中文编码的万能钥匙，经过这么多年的吐槽后Python3学乖了，默认编码是Unicode了..</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="取数据"><a href="#取数据" class="headerlink" title="取数据"></a>取数据</h4><p>爬取整个网页完毕,接下来的就是取数据过程了,首先观察</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/%E7%BD%91%E9%A1%B5%E7%BB%93%E6%9E%84.png"></p>
<p>很明显可以看出网页结构如下图所示</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>images/teacher/javaee/20220302113627师老师高级讲师2009年入行.jpg<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>li_txt<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h3</span><span class="token punctuation">></span></span>师老师<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h3</span><span class="token punctuation">></span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h4</span><span class="token punctuation">></span></span>高级讲师<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h4</span><span class="token punctuation">></span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>13年的项目开发和教育培训经验，精通Java EE的主流开发框架、Oracle和MySQL等关系型数据库。曾在中科院遥感应用研究所、慧点科技、达利本斯等公司担任软件开发工程师、项目总监，带团队做过边防部队、人寿集团、平安集团等多个企业的大型项目，之后在互联网公司知果科技担任开发经理，完成知果果网的核心产品开发。						<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>所以,直接使用xpath来提取数据</p>
<p>我们先引用<code>Itcast/items.py</code>里面的<strong>ItcastItem</strong>类</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> Itcast<span class="token punctuation">.</span>items <span class="token keyword">import</span> ItcastItem
导包从项目的根目录下开始<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>然后将我们得到的数据封装到一个<strong>Itcastitem</strong>对象中，可以保存每个老师的属性:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> Itcast<span class="token punctuation">.</span>Itcast<span class="token punctuation">.</span>items <span class="token keyword">import</span> ItcastItem


<span class="token keyword">class</span> <span class="token class-name">ItcastSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># Spider类</span>
    name <span class="token operator">=</span> <span class="token string">"itcast"</span>  <span class="token comment"># 爬虫名称</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"www.itcast.cn"</span><span class="token punctuation">]</span>  <span class="token comment"># 爬虫运行的域名</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string">"http://www.itcast.cn/channel/teacher.shtml"</span>
    <span class="token punctuation">]</span>  <span class="token comment"># 起始url,爬虫程序启动的第一次请求目的url</span>

    <span class="token triple-quoted-string string">"""处理响应数据的方法"""</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"teacher.html"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>

        <span class="token comment"># 存放老师信息的集合</span>
        items <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> each <span class="token keyword">in</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span>
            <span class="token string">"//div[@class=' tea_txt']//ul//li//div[@class='li_txt']"</span>
        <span class="token punctuation">)</span><span class="token punctuation">:</span>	<span class="token comment"># each是一个结点,对节点使用xpath方法需要加'./'表示当前节点下</span>
            
            <span class="token comment"># 将我们得到的数据封装成一个ItcastItem对象</span>
            item <span class="token operator">=</span> ItcastItem<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># extract()方法返回的都是unicode字符串</span>
            name <span class="token operator">=</span> each<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./h3/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
            title <span class="token operator">=</span> each<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./h4/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
            info <span class="token operator">=</span> each<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./p/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># xpath返回的是包含一个元素的列表</span>
            item<span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span> <span class="token operator">=</span> name<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">]</span> <span class="token operator">=</span> title<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"info"</span><span class="token punctuation">]</span> <span class="token operator">=</span> info<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

            items<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">)</span>

        <span class="token comment"># 直接返回最后数据</span>
        <span class="token keyword">return</span> items
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>我们先不处理管道,后面会详细解释</p>
<h4 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h4><p>scrapy保存信息的最简单的方法主要有四种，-o输出指定格式的文件，命令如下</p>
<pre class="line-numbers language-cmd" data-language="cmd"><code class="language-cmd">如果spiders&#x2F;itcast.py中使用return 返回的是一个ItcastItem对象,会自动识别给piplines管道来处理数据
如果return的是ItcastItem对象的列表,可以使用以下命令来进行持久化存储

# json格式，默认为Unicode编码
scrapy crawl itcast -o teachers.json
# json lines格式，默认为Unicode编码
scrapy crawl itcast -o teachers.jsonl
# csv 逗号表达式，可用Exce1打开
scrapy crawl itcast -o teachers.csv
# xml格式
scrapy crawl itcast -o teachers.xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h4><p>如果将代码改成下面形式，结果完全一样。请思考<strong>yield</strong>在这里的作用</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy

<span class="token keyword">from</span> Itcast<span class="token punctuation">.</span>items <span class="token keyword">import</span> ItcastItem


<span class="token keyword">class</span> <span class="token class-name">ItcastSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># Spider类</span>
    name <span class="token operator">=</span> <span class="token string">"itcast"</span>  <span class="token comment"># 爬虫名称</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"www.itcast.cn"</span><span class="token punctuation">]</span>  <span class="token comment"># 爬虫运行的域名</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string">"http://www.itcast.cn/channel/teacher.shtml"</span>
    <span class="token punctuation">]</span>  <span class="token comment"># 起始url,爬虫程序启动的第一次请求目的url</span>

    <span class="token triple-quoted-string string">"""处理响应数据的方法"""</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"teacher.html"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>

        <span class="token comment"># 存放老师信息的集合</span>
        <span class="token comment"># items = []</span>
        <span class="token keyword">for</span> each <span class="token keyword">in</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span>
            <span class="token string">"//div[@class=' tea_txt']//ul//li//div[@class='li_txt']/"</span>
        <span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 将我们得到的数据封装成一个ItcastItem对象</span>
            item <span class="token operator">=</span> ItcastItem<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># extract()方法返回的都是unicode字符串</span>
            name <span class="token operator">=</span> each<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./h3/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
            title <span class="token operator">=</span> each<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./h4/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
            info <span class="token operator">=</span> each<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./p/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># xpath返回的是包含一个元素的列表</span>
            item<span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span> <span class="token operator">=</span> name<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">]</span> <span class="token operator">=</span> title<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"info"</span><span class="token punctuation">]</span> <span class="token operator">=</span> info<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            
            <span class="token comment"># items.append(item)</span>
            
            <span class="token comment"># 将获取到的数据交给piplines,后继续回来执行</span>
            <span class="token keyword">yield</span> item

        <span class="token comment"># 直接返回最后数据</span>
        <span class="token comment"># return items</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><p><strong>yield</strong>的作用是,执行到yield后,不会像return那样直接返回,函数结束,而是会在返回此次后继续到上次执行的位置继续执行,<strong>return每次调用返回的值都一样</strong></p>
</li>
<li><p>带有 <strong>yield</strong> 的函数不再是一个普通函数，而是一个生成器 generator，可用于迭代</p>
</li>
<li><p>yield 是一个类似 return 的关键字，迭代一次遇到 <strong>yield</strong> 时就返回 yield 后面(右边)的值。重点是:下一次迭代时，从上一次迭代遇到的yield后面的代码(下一行)开始执行</p>
</li>
<li><p>简要理解:yield就是 return 返回一个值，并且记住这个返回的位置，下次迭代就从这个位置后(下一行)开始</p>
</li>
</ul>
<h4 id="管道处理保存数据-piplines-py"><a href="#管道处理保存数据-piplines-py" class="headerlink" title="管道处理保存数据(piplines.py)"></a>管道处理保存数据(piplines.py)</h4><p>要先在settings.py中启用管道后,itcast.py爬取的数据才会经过piplines管道进行处理保存</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Define your item pipelines here</span>
<span class="token comment">#</span>
<span class="token comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="token comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="token comment"># useful for handling different item types with a single interface</span>
<span class="token keyword">import</span> json


<span class="token keyword">class</span> <span class="token class-name">ItcastPipeline</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>f <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"itcast_piplines.json"</span><span class="token punctuation">,</span> <span class="token string">"wb"</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># item就是items.py返回的值</span>
        content <span class="token operator">=</span> json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span>
            <span class="token builtin">dict</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">,</span> ensure_ascii<span class="token operator">=</span><span class="token boolean">False</span>
        <span class="token punctuation">)</span>  <span class="token comment"># 把字典转换成json格式,ensure_ascii表示不把中文字符串当做ascii码,而是unicode</span>
        self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>content<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 返回给引擎,告诉引擎处理完毕,给我下一个</span>
        <span class="token keyword">return</span> item

    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 整个爬虫程序关闭时做的事</span>
        self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
 	<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>需要在**<code>items.py</code>**中定义需要的属性</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ItcastItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># define the fields for your item here like:</span>
    name <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># Field()类似字典</span>
    info <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>  
    title <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>  
    <span class="token comment"># pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h2 id="Item-Pipline"><a href="#Item-Pipline" class="headerlink" title="Item Pipline"></a>Item Pipline</h2><p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline,这些Item Pipeline组件按定义的顺序处理Item.<br>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用:</p>
<ul>
<li>验证爬取的数据(检查item包含某些字段，比如说name字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到文件或者数据库中</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> something  <span class="token comment"># 确保这里的something是您实际想要导入的模块或包  </span>
  
<span class="token keyword">class</span> <span class="token class-name">SomethingPipeline</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token comment"># 可选实现，做参数初始化等  </span>
        <span class="token comment"># 这里可以添加初始化代码，例如连接数据库等  </span>
        <span class="token keyword">pass</span>  
    
	<span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token comment"># item(Item 对象) - 被爬取的item  </span>
        <span class="token comment"># spider(Spider 对象) - 爬取该item的spider  </span>
        <span class="token comment"># 这个方法必须实现，每个item pipeline组件都需要调用该方法  </span>
        <span class="token comment"># 方法必须返回一个 Item 对象，被丢弃的item将不会被之后的pipeline组件处理  </span>
        <span class="token comment"># 这里可以添加处理item的代码，比如清洗数据、保存数据到数据库等  </span>
        <span class="token keyword">return</span> item
    
    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token comment"># spider(Spider 对象) - 被开启的spider  </span>
        <span class="token comment"># 可选实现，当spider被开启时，这个方法被调用  </span>
        <span class="token comment"># 这里可以添加一些在spider启动时需要的初始化操作  </span>
        <span class="token keyword">pass</span>  
    
    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token comment"># spider(Spider 对象) - 被关闭的spider  </span>
        <span class="token comment"># 可选实现，当spider被关闭时，这个方法被调用  </span>
        <span class="token comment"># 这里可以添加一些清理代码，比如关闭数据库连接等  </span>
        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h2 id="Scrapy-Shell"><a href="#Scrapy-Shell" class="headerlink" title="Scrapy Shell"></a>Scrapy Shell</h2><p><strong>Scrapy终端</strong>是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试<strong>XPath</strong>或<strong>CSS</strong>表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。</p>
<p>如果安装了IPython，Scrapy终端将使用 IPython(替代标准Python终端)。<strong>IPython</strong>终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。(推荐安装IPython)</p>
<h3 id="启动scrapyshell"><a href="#启动scrapyshell" class="headerlink" title="启动scrapyshell"></a>启动scrapyshell</h3><p>进入项目的根目录，执行下列命令来启动shell</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scrapy shell <span class="token string">"http://ww.itcast.cn/channel/teacher.shtml"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/image-20240803112644048.png" alt="image-20240803112644048"></p>
<p><strong>Scrapy Shell</strong>根据下载的页面会自动创建一些方便使用的对象，例如<code>Response</code>对象，以及<code>selector</code>对象(对HTML及XML内容)。</p>
<p>当shell载入后，将得到一个包含response数据的本地response变量，输入<code>response.body</code>将输出response的包体，输出response.headers 可以看到response的包头。</p>
<p>输入response.selector 时，将获取到一个response 初始化的类 Selector 的对象,此时可以通过使用 <code>response.selector.xpath()</code>或<code>response.selector.css()</code>来对 response 进行查询。</p>
<p>Scrapy也提供了一些快捷方式,例如 <code>response.xpath()</code>或<code> response.css()</code>同样可以生效(如之前的案例)。</p>
<h3 id="Selectors选择器"><a href="#Selectors选择器" class="headerlink" title="Selectors选择器"></a>Selectors选择器</h3><p><strong>Scrapy Selectors</strong> 内置 <strong>XPath</strong> 和 <strong>Css Selector</strong> 表达式机制</p>
<p>Selector有四个基本的方法，最常用的还是xpath</p>
<ul>
<li><code>xpath()</code>:传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</li>
<li><code>extract()</code>:序列化该节点为Unicode字符串并回list</li>
<li><code>css()</code>:传入CSS表达式，返回该表达式所对应的所有节点的selector list列表,语法同 BeautifulSoup4。</li>
<li><code>re()</code>:根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表</li>
</ul>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt"># 定位页面上的所有链接  
//a  
  
# 定位具有特定类名的元素，例如所有类名为"my-class"的&lt;div>元素  
//div[@class='my-class']  
  
# 如果类名有多个值，定位包含特定类名的元素  
//div[contains(concat(' ', normalize-space(@class), ' '), ' my-class ')]  
  
# 定位包含特定文本的元素，例如所有包含"点击这里"文本的&lt;a>元素  
//a[contains(text(), '点击这里')]  
  
# 定位具有特定属性值的元素，例如所有href属性中包含"example.com"的&lt;a>元素  
//a[contains(@href, 'example.com')]  
  
# 定位某个特定元素下的所有直接子元素，例如&lt;div id="container">下的所有&lt;p>元素  
//div[@id='container']/p  
  
# 定位具有特定id的元素，例如id为"unique-id"的&lt;div>元素  
//div[@id='unique-id']<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h2><p><strong>Spider</strong>类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 </p>
<p>换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</p>
<p><code>class scrapy.spider</code> 是最基本的类，所有编写的爬虫必须继承这个类。</p>
<p>主要用到的函数及调用顺序为:</p>
<ul>
<li><code>init_()</code>: 初始化爬虫名字和start_urls列表</li>
<li><code>start_requests()</code>: 调用<code>make_requests_from_url()</code>生成Requests对象交给Scrapy下载并返response </li>
<li><code>parse()</code>: 解析response,并返回 Item 或 Requests (需指定回调函数)。Item传给<strong>item pipline</strong>持久化 ，而Requests交由Scrapy下载，并由指定的回调函数处理(默认parse()，一直进行循环，直到处理完所有的数据为止。)</li>
</ul>
<h3 id="源码参考"><a href="#源码参考" class="headerlink" title="源码参考"></a>源码参考</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 所有爬虫的基类，用户定义的爬虫必须从这个类继承</span>
<span class="token keyword">class</span> <span class="token class-name">Spider</span><span class="token punctuation">(</span>object_ref<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Base class for scrapy spiders. All spiders must inherit from this class."""</span>

    name<span class="token punctuation">:</span> <span class="token builtin">str</span>
    custom_settings<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">dict</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
    
   <span class="token comment"># 定义spider名字的字符串(string)。spider的名字定义了scrapy如何定位(并初始化)spider，所以其必须是唯一的</span>
   <span class="token comment"># name是spider最重要的属性，而且是必须的。</span>
	<span class="token comment"># 一般做法是以该网站(domain)(加或不加后缀)来命名spider。例如，如果spider爬取mywebsite.com，该spider名为mywebsite</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> name <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>name <span class="token operator">=</span> name
        <span class="token comment"># 如果爬虫没有名字，中断后续操作则报错</span>
        <span class="token keyword">elif</span> <span class="token keyword">not</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"name"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">type</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__name__<span class="token punctuation">&#125;</span></span><span class="token string"> must have a name"</span></span><span class="token punctuation">)</span>
            
        <span class="token comment"># python 对象或类型通过内置成员_dict 来存储成员信息</span>
        self<span class="token punctuation">.</span>__dict__<span class="token punctuation">.</span>update<span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>
        
        <span class="token comment"># URL列表。当没有指定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表的内容</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"start_urls"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>start_urls<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">logger</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> logging<span class="token punctuation">.</span>LoggerAdapter<span class="token punctuation">:</span>
        logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span>self<span class="token punctuation">.</span>name<span class="token punctuation">)</span>
        <span class="token keyword">return</span> logging<span class="token punctuation">.</span>LoggerAdapter<span class="token punctuation">(</span>logger<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">"spider"</span><span class="token punctuation">:</span> self<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>

    <span class="token comment"># 打印scrapy执行后的1og信息</span>
    <span class="token keyword">def</span> <span class="token function">log</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> message<span class="token punctuation">:</span> Any<span class="token punctuation">,</span> level<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">,</span> <span class="token operator">**</span>kw<span class="token punctuation">:</span> Any<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Log the given message at the given log level

        This helper wraps a log call to the logger within the spider, but you
        can use it directly (e.g. Spider.logger.info('msg')) or use any other
        Python logger too.
        """</span>
        self<span class="token punctuation">.</span>logger<span class="token punctuation">.</span>log<span class="token punctuation">(</span>level<span class="token punctuation">,</span> message<span class="token punctuation">,</span> <span class="token operator">**</span>kw<span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> crawler<span class="token punctuation">:</span> Crawler<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">:</span> Any<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Self<span class="token punctuation">:</span>
        spider <span class="token operator">=</span> cls<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        spider<span class="token punctuation">.</span>_set_crawler<span class="token punctuation">(</span>crawler<span class="token punctuation">)</span>
        <span class="token keyword">return</span> spider

    <span class="token comment"># 判断对象object的属性是否存在，不存在做断言处理</span>
    <span class="token keyword">def</span> <span class="token function">_set_crawler</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> crawler<span class="token punctuation">:</span> Crawler<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>crawler <span class="token operator">=</span> crawler
        self<span class="token punctuation">.</span>settings <span class="token operator">=</span> crawler<span class="token punctuation">.</span>settings
        crawler<span class="token punctuation">.</span>signals<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>self<span class="token punctuation">.</span>close<span class="token punctuation">,</span> signals<span class="token punctuation">.</span>spider_closed<span class="token punctuation">)</span>

        
    <span class="token comment"># 该方法将读取start_urls内的地址，并为每一个地址生成一个Request对象，交给Scrapy下载并返回Response#该方法仅调用一次</span>
    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Iterable<span class="token punctuation">[</span>Request<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>start_urls <span class="token keyword">and</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"start_url"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> AttributeError<span class="token punctuation">(</span>
                <span class="token string">"Crawling could not start: 'start_urls' not found "</span>
                <span class="token string">"or empty (but found 'start_url' attribute instead, "</span>
                <span class="token string">"did you miss an 's'?)"</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">for</span> url <span class="token keyword">in</span> self<span class="token punctuation">.</span>start_urls<span class="token punctuation">:</span>
            <span class="token keyword">yield</span> self<span class="token punctuation">.</span>make_requests_from_url<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
            
    <span class="token keyword">def</span>  <span class="token function">make_requests_from_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>url<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#start_requests()中调用，实际生成Request的函数。</span>
        <span class="token comment">#Request对象默认的回调函数为parse()，提交的方式为get</span>
       	<span class="token keyword">return</span> Request<span class="token punctuation">(</span>url<span class="token punctuation">,</span> dont_filter<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">:</span> Response<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Any<span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>response<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    
    <span class="token comment"># 默认的Request对象回调函数，处理返回的response.</span>
    <span class="token comment"># 生成Item或者Request对象。用户必须实现这个类</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">:</span> Response<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Any<span class="token punctuation">:</span>
        <span class="token keyword">raise</span> NotImplementedError<span class="token punctuation">(</span>
            <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">&#125;</span></span><span class="token string">.parse callback is not defined"</span></span>
        <span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">update_settings</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> settings<span class="token punctuation">:</span> BaseSettings<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        settings<span class="token punctuation">.</span>setdict<span class="token punctuation">(</span>cls<span class="token punctuation">.</span>custom_settings <span class="token keyword">or</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> priority<span class="token operator">=</span><span class="token string">"spider"</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">handles_request</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> request<span class="token punctuation">:</span> Request<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">bool</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> url_is_from_spider<span class="token punctuation">(</span>request<span class="token punctuation">.</span>url<span class="token punctuation">,</span> cls<span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">close</span><span class="token punctuation">(</span>spider<span class="token punctuation">:</span> Spider<span class="token punctuation">,</span> reason<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Union<span class="token punctuation">[</span>Deferred<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        closed <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>spider<span class="token punctuation">,</span> <span class="token string">"closed"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">callable</span><span class="token punctuation">(</span>closed<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> cast<span class="token punctuation">(</span>Union<span class="token punctuation">[</span>Deferred<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">,</span> closed<span class="token punctuation">(</span>reason<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">__repr__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"&lt;</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">type</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__name__<span class="token punctuation">&#125;</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>name<span class="token conversion-option punctuation">!r</span><span class="token punctuation">&#125;</span></span><span class="token string"> at 0x</span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">id</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">0x</span><span class="token punctuation">&#125;</span></span><span class="token string">>"</span></span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jira/p/16574364.html">Python @property装饰器详解 - 贾志文 - 博客园 (cnblogs.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhh763984017/article/details/120072425">【Python】一文弄懂python装饰器（附源码例子）_python 装饰器-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/w3cnote/python-func-decorators.html">Python 函数装饰器 | 菜鸟教程 (runoob.com)</a></p>
<ul>
<li><p><strong>name</strong>	定义spider名字的字符串。</p>
<p>  例如，如果spider爬取 mywebsite.com，该spider通常会被命名为 mywebsite</p>
</li>
<li><p><strong>allowed domains</strong></p>
<p>  包含了spider允许爬取的域名(domain)的列表，可选。</p>
</li>
<li><p><strong>start urls</strong></p>
<p>  初始URL元组&#x2F;列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。</p>
</li>
<li><p><strong>start requests(self)</strong></p>
<p>该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取(默认实现是使用start urls 的</p>
<p>url)的第一个Request.当spider启动爬取并且未指定start urls时，该方法被调用。</p>
</li>
<li><p><strong>parse(self, response)</strong></p>
<p>  当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。</p>
</li>
<li><p><strong>log(self,messagel.level, component])</strong></p>
<p>  使用 scrapy.log.msg()方法记录(log)message。更多数据请参见 logging</p>
</li>
</ul>
<h3 id="尝试腾讯招聘案例-普通版"><a href="#尝试腾讯招聘案例-普通版" class="headerlink" title="尝试腾讯招聘案例(普通版)"></a>尝试腾讯招聘案例(普通版)</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hwwaizs/article/details/120392605">python爬虫（二十二）scrapy案例–爬取腾讯招聘数据_爬取腾讯社会招聘“数据分析”岗位的所有招聘信息-CSDN博客</a></p>
<p>我们用腾讯社招的网站<a target="_blank" rel="noopener" href="https://careers.tencent.com/search.html?index=1">搜索 | 腾讯招聘 (tencent.com)</a>举例:</p>
<p>(<a target="_blank" rel="noopener" href="http://hr.tencent.com/position.php?&start=0#a%E4%B8%AD#a%E8%A1%A8%E7%A4%BA%E9%94%9A%E7%82%B9,%E5%AE%9A%E4%BD%8D%E5%88%B0%E9%A1%B5%E9%9D%A2%E7%9A%84%E5%93%AA%E4%B8%AA%E4%BD%8D%E7%BD%AE">http://hr.tencent.com/position.php?&amp;start=0#a中#a表示锚点,定位到页面的哪个位置</a>)</p>
<h4 id="首先分析要爬取的数据有哪些"><a href="#首先分析要爬取的数据有哪些" class="headerlink" title="首先分析要爬取的数据有哪些"></a>首先分析要爬取的数据有哪些</h4><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/分析要爬取的数据.png" alt="image-20240803133142140">

<p>根节点的xpath路径为<code>//div[@class=&#39;recruit-list&#39;]/a</code></p>
<p><strong>初步确定要爬取的信息</strong></p>
<p>职位名positionName: <code>//div[@class=&#39;recruit-list&#39;]/a/div/span[1]</code></p>
<p>职位类别positionType: <code>//div[@class=&#39;recruit-list&#39;]/a/p/span[3]</code></p>
<p>职位要求positionRequire:<code>//div[@class=&#39;recruit-list&#39;]/a/p/span[5]</code></p>
<p>工作地点workLocation: <code>//div[@class=&#39;recruit-list&#39;]/a/div/span[2]</code></p>
<p>详细介绍positionInfo: <code>//div[@class=&#39;recruit-list&#39;]/a/p[@class=&#39;recruit-text&#39;]</code></p>
<p>最后更新时间updateTime:<code>//div[@class=&#39;recruit-list&#39;]/a/p/span[7]</code></p>
<h4 id="然后创建爬虫项目和程序"><a href="#然后创建爬虫项目和程序" class="headerlink" title="然后创建爬虫项目和程序"></a>然后创建爬虫项目和程序</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">scrapy startproject Tencent
 <span class="token builtin class-name">cd</span> .<span class="token punctuation">\</span>Tencent<span class="token punctuation">\</span>
scrapy genspider  tencent <span class="token string">"tencent.com"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h4 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h4><p>定义要采集的数据模型</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Define here the models for your scraped items</span>
<span class="token comment">#</span>
<span class="token comment"># See documentation in:</span>
<span class="token comment"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">TencentItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># define the fields for your item here like:</span>
    <span class="token comment"># name = scrapy.Field()</span>
    <span class="token comment"># 职位名</span>
    positionName <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 职位类别</span>
    positionType <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 职位要求</span>
    positionRequire <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 工作地点</span>
    workLocation <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 详细介绍</span>
    positionInfo <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 最后更新时间</span>
    updateTime <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="tencent-py"><a href="#tencent-py" class="headerlink" title="tencent.py"></a>tencent.py</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> Tencent<span class="token punctuation">.</span>items <span class="token keyword">import</span> TencentItem


<span class="token keyword">class</span> <span class="token class-name">TencentSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"tencent"</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"tencent.com"</span><span class="token punctuation">]</span>
    baseUrl <span class="token operator">=</span> <span class="token string">"http://hr.tencent.com/search.html?index="</span>
    index <span class="token operator">=</span> <span class="token number">1</span>
    start_url <span class="token operator">=</span> <span class="token punctuation">[</span>baseUrl <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>

        node_list <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//div[@class='recruit-list']/a"</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> node <span class="token keyword">in</span> node_list<span class="token punctuation">:</span>
            item <span class="token operator">=</span> TencentItem<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># 提取每个职位的信息</span>
            item<span class="token punctuation">[</span><span class="token string">"positionName"</span><span class="token punctuation">]</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./div/span[1]/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"positionType"</span><span class="token punctuation">]</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span>
                <span class="token string">"//div[@class='recruit-list']/a/p/span[3]/text()"</span>
            <span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"positionRequire"</span><span class="token punctuation">]</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span>
                <span class="token string">"//div[@class='recruit-list']/a/p/span[5]/text()"</span>
            <span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"workLocation"</span><span class="token punctuation">]</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./div/span[2]/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"positionInfo"</span><span class="token punctuation">]</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span>
                <span class="token string">"//div[@class='recruit-list']/a/p[@class='recruit-text']/text()"</span>
            <span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"updateTime"</span><span class="token punctuation">]</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span>
                <span class="token string">"//div[@class='recruit-list']/a/p/span[7]/text()"</span>
            <span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

            <span class="token keyword">yield</span> item

         <span class="token comment"># 此次硬编码,不是最佳方案,可在每一页中提取下一页url</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>index <span class="token operator">&lt;</span> <span class="token number">282</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>index <span class="token operator">+=</span> <span class="token number">10</span>
            url <span class="token operator">=</span> self<span class="token punctuation">.</span>baseUrl <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>index<span class="token punctuation">)</span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>

    <span class="token comment"># def parse_next(self,response):</span>
    <span class="token comment">#     pass</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><code>start_url = [baseUrl + str(index)]</code>:拼接请求网址</li>
<li><code>yield scrapy.Request(url=url, callback=self.parse)</code>: 对新网址发送请求,callback函数可自己定义</li>
<li>yield会把item&#x2F;Request返回给引擎,引擎判断是交给管道保存还是继续进入请求队列</li>
</ul>
<h4 id="piplines-py"><a href="#piplines-py" class="headerlink" title="piplines.py"></a>piplines.py</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Define your item pipelines here</span>
<span class="token comment">#</span>
<span class="token comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="token comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="token comment"># useful for handling different item types with a single interface</span>
<span class="token keyword">import</span> json
<span class="token keyword">from</span> itemadapter <span class="token keyword">import</span> ItemAdapter


<span class="token keyword">class</span> <span class="token class-name">TencentPipeline</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>f <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"data.json"</span><span class="token punctuation">,</span> <span class="token string">"wb"</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n"</span> <span class="token operator">+</span> item<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>
            <span class="token punctuation">(</span>json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">,</span> ensure_ascii<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">",\n"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">return</span> item

    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="腾讯招聘-抓包"><a href="#腾讯招聘-抓包" class="headerlink" title="腾讯招聘(抓包)"></a>腾讯招聘(抓包)</h3><h4 id="什么是抓包"><a href="#什么是抓包" class="headerlink" title="什么是抓包"></a>什么是抓包</h4><p>在网络爬虫的上下文中，抓包技术可以被用来分析和优化爬虫的性能。具体来说，爬虫开发者可以使用抓包工具（如</p>
<p>Wireshark、tcpdump等）来捕获爬虫程序与服务器之间的通信数据包。通过对这些数据包的分析，开发者可以了解爬虫</p>
<p>请求的发送情况、服务器的响应情况，以及请求和响应中携带的具体数据内容。</p>
<h4 id="分析请求"><a href="#分析请求" class="headerlink" title="分析请求"></a>分析请求</h4><p>打开腾讯招聘页面,右键源代码,随便搜索一个网页中的职位名称,结果为空,说明,职位信息是动态响应到界面中的;</p>
<p>此时,就要到网络中去抓包,按F11打开调试工具,点到网络项,XHR,然后刷新界面,逐个检查发现<code>Query</code>开头的请求返回的响应数据(预览)中有我们想要的数据.</p>
<p>复制请求url,在另一窗口打开,不断删除参数,最后发现,只需要简单的<code>https://careers.tencent.com/tencentcareer/api/post/Query?pageIndex=281&amp;pageSize=10</code>就能访问到数据,再将相隔两页对比数据,发现<strong>pageIndex的值</strong>就是当前页面的索引,<strong>pageSize固定为10</strong>(每页展示的招聘信息总数)</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/%E6%8A%93%E5%8C%85%E4%B9%8Bxhr.png" alt="image-20240804094111942"></p>
<p>所以,我们试探性的将pageIndex改为1,果然发现获取到了第一页的招聘信息</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/%E7%AE%80%E5%8C%96url.png" alt="image-20240804094920088"></p>
<h4 id="爬取数据"><a href="#爬取数据" class="headerlink" title="爬取数据"></a>爬取数据</h4><p>把上面请求的响应数据放到json解析器中去,解析想要的数据所处的格式</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/json%E6%A0%BC%E5%BC%8F%E7%9A%84%E5%93%8D%E5%BA%94.png" alt="image-20240804095531422"></p>
<h4 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h4><p>在<code>spiders/tencent.py</code>中</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> json
<span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> Tencent<span class="token punctuation">.</span>items <span class="token keyword">import</span> TencentItem


<span class="token keyword">class</span> <span class="token class-name">TencentSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"tencent"</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"tencent.com"</span><span class="token punctuation">]</span>
    index <span class="token operator">=</span> <span class="token number">1</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string-interpolation"><span class="token string">f"http://careers.tencent.com/tencentcareer/api/post/Query?pageIndex=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>index<span class="token punctuation">&#125;</span></span><span class="token string">&amp;pageSize=10"</span></span>
    <span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"======================================"</span><span class="token punctuation">)</span>	<span class="token comment"># 输出一行,让你更容易看到输出信息,后面写完项目可以注释掉</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>	<span class="token comment"># 返回json数据</span>
        data_dict <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>	<span class="token comment"># 把json数据变成python字典类型</span>
        RecruitPostName <span class="token operator">=</span> data_dict<span class="token punctuation">[</span><span class="token string">"Data"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"Posts"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"RecruitPostName"</span><span class="token punctuation">]</span>	<span class="token comment"># 解嵌套</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span>RecruitPostName<span class="token punctuation">)</span>	
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"======================================"</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>经过一系列操作,最后成功获得想要的第一个职位信息</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/%E7%AC%AC%E4%B8%80%E4%B8%AA%E8%81%8C%E4%BD%8D%E4%BF%A1%E6%81%AF.png" alt="image-20240804102136386"></p>
<p>最终代码为</p>
<h5 id="tencent-py-1"><a href="#tencent-py-1" class="headerlink" title="tencent.py"></a>tencent.py</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> json
<span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> Tencent<span class="token punctuation">.</span>items <span class="token keyword">import</span> TencentItem


<span class="token keyword">class</span> <span class="token class-name">TencentSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"tencent"</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"tencent.com"</span><span class="token punctuation">]</span>
    index <span class="token operator">=</span> <span class="token number">1</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string-interpolation"><span class="token string">f"http://careers.tencent.com/tencentcareer/api/post/Query?pageIndex=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>index<span class="token punctuation">&#125;</span></span><span class="token string">&amp;pageSize=10"</span></span>
    <span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
        data_dict <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>
        datas <span class="token operator">=</span> data_dict<span class="token punctuation">[</span><span class="token string">"Data"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"Posts"</span><span class="token punctuation">]</span>  <span class="token comment"># 返回一个数组</span>

        <span class="token keyword">for</span> data <span class="token keyword">in</span> datas<span class="token punctuation">:</span>
            item <span class="token operator">=</span> TencentItem<span class="token punctuation">(</span><span class="token punctuation">)</span>
            item<span class="token punctuation">[</span><span class="token string">"positionName"</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"RecruitPostName"</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"workLocation"</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"LocationName"</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"positionType"</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"CategoryName"</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"positionInfo"</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"Responsibility"</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"PostURL"</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"PostURL"</span><span class="token punctuation">]</span>
            item<span class="token punctuation">[</span><span class="token string">"updateTime"</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">"LastUpdateTime"</span><span class="token punctuation">]</span>

            <span class="token keyword">yield</span> item

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>index <span class="token operator">&lt;</span> <span class="token number">282</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>index <span class="token operator">+=</span> <span class="token number">1</span>
            url <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"http://careers.tencent.com/tencentcareer/api/post/Query?pageIndex=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>index<span class="token punctuation">&#125;</span></span><span class="token string">&amp;pageSize=10"</span></span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>


<span class="token comment"># def parse_next(self,response):</span>
<span class="token comment">#     pass</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="items-py-1"><a href="#items-py-1" class="headerlink" title="items.py"></a>items.py</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Define here the models for your scraped items</span>
<span class="token comment">#</span>
<span class="token comment"># See documentation in:</span>
<span class="token comment"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">TencentItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># define the fields for your item here like:</span>
    <span class="token comment"># name = scrapy.Field()</span>
    <span class="token comment"># 职位名</span>
    positionName <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 职位类别</span>
    positionType <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 职位详情url</span>
    PostURL <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 工作地点</span>
    workLocation <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 详细介绍</span>
    positionInfo <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 最后更新时间</span>
    updateTime <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="piplines-py-1"><a href="#piplines-py-1" class="headerlink" title="piplines.py"></a>piplines.py</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Define your item pipelines here</span>
<span class="token comment">#</span>
<span class="token comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="token comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="token comment"># useful for handling different item types with a single interface</span>
<span class="token keyword">import</span> json
<span class="token keyword">from</span> itemadapter <span class="token keyword">import</span> ItemAdapter


<span class="token keyword">class</span> <span class="token class-name">TencentPipeline</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>f <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"data.json"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token punctuation">(</span>json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">,</span> ensure_ascii<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">",\n"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"成功写入</span><span class="token interpolation"><span class="token punctuation">&#123;</span>x<span class="token punctuation">&#125;</span></span><span class="token string">个字符"</span></span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> item

        <span class="token comment"># def close_spider(self, spider):</span>
        self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="运行结果展示"><a href="#运行结果展示" class="headerlink" title="运行结果展示"></a>运行结果展示</h5><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA.png" alt="image-20240804103841776"></p>
<h2 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h2><h3 id="独门秘笈"><a href="#独门秘笈" class="headerlink" title="独门秘笈"></a>独门秘笈</h3><ol>
<li><p>继承自scrapy.Spider</p>
</li>
<li><p>独门秘笈<br> CrawlSpider可以定义规则，再解析html内容的时候，可以根据链接规则提取出指定的链接，然后再向这些链接发</p>
<p> 送请求所以，如果有需要跟进链接的需求，意思就是爬取了网页之后，需要提取链接再次爬取，使用<strong>CrawlSpider</strong></p>
<p> 是非常合适的</p>
</li>
<li><p>提取链接<br> 链接提取器，在这里就可以写规则提取指定链接    </p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python">scrapy<span class="token punctuation">.</span>linkextractors<span class="token punctuation">.</span>LinkExtractor<span class="token punctuation">(</span> 
    <span class="token comment"># 正则表达式  提取符合正则的链接</span>
    allow <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    <span class="token comment"># (不用)正则表达式  不提取符合正则的链接</span>
    deny <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     
    <span class="token comment">#（不用）允许的域名  </span>
    allow_domains <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  
    <span class="token comment">#（不用）不允许的域名   </span>
    deny_domains <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># xpath，提取符合xpath规则的链接</span>
    restrict_xpaths <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      
    <span class="token comment"># 提取符合选择器规则的链接</span>
    restrict_css <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span>         
   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>模拟使用</p>
<ul>
<li>正则用法:<code>links1 = LinkExtractor(allow=r&#39;list 23 \d+\.html&#39;)</code></li>
<li>xpath用法:<code>links2 = LinkExtractor(restrict xpaths=r&#39;//div[@class=&quot;x&quot;]&#39;)</code></li>
<li>css用法:<code>links3 = LinkExtractor(restrict css=&#39;.x&#39;)</code></li>
</ul>
</li>
<li><p>提取连接<br> <code>link.extract_links(response)</code></p>
</li>
<li><p>注意事项<br> 【注1】callback只能写函数名字符串，<strong>callback&#x3D;’parse item’</strong><br> 【注2】在基本的spider中，如果重新发送请求，那里的callback写的是callback&#x3D;self.parse_item</p>
</li>
<li><p>follow&#x3D;true 是否跟进 就是按照提取连接规则进行提取</p>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2024/08/04/scrapy-kuang-jia/follow%E8%A7%84%E5%88%99.png" alt="image-20240804110723859"></p>
<h2 id="CrawlSpider案例"><a href="#CrawlSpider案例" class="headerlink" title="CrawlSpider案例"></a>CrawlSpider案例</h2><h3 id="read-py"><a href="#read-py" class="headerlink" title="read.py"></a><code>read.py</code></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>linkextractors <span class="token keyword">import</span> LinkExtractor
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>spiders <span class="token keyword">import</span> CrawlSpider<span class="token punctuation">,</span> Rule
<span class="token keyword">from</span> scrapy_dushuwang<span class="token punctuation">.</span>items <span class="token keyword">import</span> ScrapyDushuwangItem


<span class="token keyword">class</span> <span class="token class-name">ReadSpider</span><span class="token punctuation">(</span>CrawlSpider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"read"</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"www.dushu.com"</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"https://www.dushu.com/book/1107.html"</span><span class="token punctuation">]</span>
    <span class="token comment"># 正则表达式r"/book/\d+\.html"</span>
    rules <span class="token operator">=</span> <span class="token punctuation">(</span>Rule<span class="token punctuation">(</span>LinkExtractor<span class="token punctuation">(</span>allow<span class="token operator">=</span><span class="token string">r"/book/\d+\.html"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  callback<span class="token operator">=</span><span class="token string">"parse_item"</span><span class="token punctuation">,</span>
                  follow<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># follow是要不要继续提取</span>

    <span class="token keyword">def</span> <span class="token function">parse_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"======================"</span><span class="token punctuation">)</span>
        <span class="token comment"># 第一张图片没有懒加载,所以要加入图片源</span>
        <span class="token comment"># img_list[0] = "https://a.dushu.com/img/n142.png"</span>
        img_list <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="bookslist"]/ul//img/@data-original'</span><span class="token punctuation">)</span>
        name_list <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="bookslist"]/ul//a/@title'</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> item <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>img_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> item <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                name <span class="token operator">=</span> name_list<span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
                src <span class="token operator">=</span> <span class="token string">"https://a.dushu.com/img/n142.png"</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                name <span class="token operator">=</span> name_list<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
                src <span class="token operator">=</span> img_list<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>

            book <span class="token operator">=</span> ScrapyDushuwangItem<span class="token punctuation">(</span>name<span class="token operator">=</span>name<span class="token punctuation">,</span> src<span class="token operator">=</span>src<span class="token punctuation">)</span>
            <span class="token keyword">yield</span> book
<span class="token comment"># item = &#123;&#125; # 建议使用在items文件中定义数据结构</span>
<span class="token comment"># # item["domain_id"] = response.xpath('//input[@id="sid"]/@value').get()</span>
<span class="token comment"># # item["name"] = response.xpath('//div[@id="name"]').get()</span>
<span class="token comment"># # item["description"] = response.xpath('//div[@id="description"]').get()</span>
<span class="token comment"># return item</span>

<span class="token comment"># 创建新的请求并重新执行提取</span>
<span class="token comment"># next_page = response.xpath('//a[contains(text(), "下一页")]/@href').get()  # 获取下一页的链接</span>
<span class="token comment"># if next_page:</span>
<span class="token comment"># yield scrapy.Request(next_page, callback=self.parse_item)  # 重新执行提取，回调函数为 self.parse_item</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="items-py-2"><a href="#items-py-2" class="headerlink" title="items.py"></a><code>items.py</code></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Define here the models for your scraped items</span>
<span class="token comment">#</span>
<span class="token comment"># See documentation in:</span>
<span class="token comment"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">ScrapyDushuwangItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># define the fields for your item here like:</span>
    <span class="token comment"># name = scrapy.Field()</span>
    name <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    src <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="piplines-py-2"><a href="#piplines-py-2" class="headerlink" title="piplines.py"></a><code>piplines.py</code></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Define your item pipelines here</span>
<span class="token comment">#</span>
<span class="token comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="token comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="token comment"># useful for handling different item types with a single interface</span>


<span class="token keyword">class</span> <span class="token class-name">ScrapyDushuwangPipeline</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fp <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'book.json'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fp<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> item

    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fp<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">import</span> pymysql
<span class="token comment"># 加载settings文件</span>
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>project <span class="token keyword">import</span> get_project_settings


<span class="token keyword">class</span> <span class="token class-name">MySqlPipline</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        settings <span class="token operator">=</span> get_project_settings<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>host <span class="token operator">=</span> settings<span class="token punctuation">[</span><span class="token string">'DB_HOST'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>port <span class="token operator">=</span> settings<span class="token punctuation">[</span><span class="token string">'DB_PORT'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>user <span class="token operator">=</span> settings<span class="token punctuation">[</span><span class="token string">'DB_USER'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>password <span class="token operator">=</span> settings<span class="token punctuation">[</span><span class="token string">'DB_PASSWORD'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>name <span class="token operator">=</span> settings<span class="token punctuation">[</span><span class="token string">'DB_NAME'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>charset <span class="token operator">=</span> settings<span class="token punctuation">[</span><span class="token string">'DB_CHARSET'</span><span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>connect<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">connect</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>conn <span class="token operator">=</span> pymysql<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>
            host<span class="token operator">=</span>self<span class="token punctuation">.</span>host<span class="token punctuation">,</span>
            port<span class="token operator">=</span>self<span class="token punctuation">.</span>port<span class="token punctuation">,</span>
            user<span class="token operator">=</span>self<span class="token punctuation">.</span>user<span class="token punctuation">,</span>
            password<span class="token operator">=</span>self<span class="token punctuation">.</span>password<span class="token punctuation">,</span>
            db<span class="token operator">=</span>self<span class="token punctuation">.</span>name<span class="token punctuation">,</span>
            charset<span class="token operator">=</span>self<span class="token punctuation">.</span>charset
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>cursor <span class="token operator">=</span> self<span class="token punctuation">.</span>conn<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        sql <span class="token operator">=</span> <span class="token string">'insert into book(name,src) values("&#123;&#125;","&#123;&#125;")'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'name'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> item<span class="token punctuation">[</span><span class="token string">'src'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 执行sql语句</span>
        self<span class="token punctuation">.</span>cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span>sql<span class="token punctuation">)</span>
        <span class="token comment"># 提交</span>
        self<span class="token punctuation">.</span>conn<span class="token punctuation">.</span>commit<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> item

    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>cursor<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conn<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a><code>settings.py</code></h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Scrapy settings for scrapy_dushuwang project</span>
<span class="token comment">#</span>
<span class="token comment"># For simplicity, this file contains only settings considered important or</span>
<span class="token comment"># commonly used. You can find more settings consulting the documentation:</span>
<span class="token comment">#</span>
<span class="token comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span>
<span class="token comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span class="token comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>

BOT_NAME <span class="token operator">=</span> <span class="token string">"scrapy_dushuwang"</span>

SPIDER_MODULES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"scrapy_dushuwang.spiders"</span><span class="token punctuation">]</span>
NEWSPIDER_MODULE <span class="token operator">=</span> <span class="token string">"scrapy_dushuwang.spiders"</span>

<span class="token comment"># 连接mysql参数</span>
DB_HOST <span class="token operator">=</span> <span class="token string">'127.0.0.1'</span>
<span class="token comment"># DB_HOST = 'localhost'</span>
DB_PORT <span class="token operator">=</span> <span class="token number">3306</span>
DB_USER <span class="token operator">=</span> <span class="token string">'root'</span>
DB_PASSWORD <span class="token operator">=</span> <span class="token string">'Xubin159753123'</span>
DB_NAME <span class="token operator">=</span> <span class="token string">"spider01"</span>
<span class="token comment"># 注意utf-8的-不允许写</span>
DB_CHARSET <span class="token operator">=</span> <span class="token string">'utf8'</span>

<span class="token comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span>
<span class="token comment"># USER_AGENT = "scrapy_dushuwang (+http://www.yourdomain.com)"</span>

<span class="token comment"># Obey robots.txt rules</span>
ROBOTSTXT_OBEY <span class="token operator">=</span> <span class="token boolean">True</span>

<span class="token comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span>
<span class="token comment"># CONCURRENT_REQUESTS = 32</span>

<span class="token comment"># Configure a delay for requests for the same website (default: 0)</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span>
<span class="token comment"># See also autothrottle settings and docs</span>
<span class="token comment"># DOWNLOAD_DELAY = 3</span>
<span class="token comment"># The download delay setting will honor only one of:</span>
<span class="token comment"># CONCURRENT_REQUESTS_PER_DOMAIN = 16</span>
<span class="token comment"># CONCURRENT_REQUESTS_PER_IP = 16</span>

<span class="token comment"># Disable cookies (enabled by default)</span>
<span class="token comment"># COOKIES_ENABLED = False</span>

<span class="token comment"># Disable Telnet Console (enabled by default)</span>
<span class="token comment"># TELNETCONSOLE_ENABLED = False</span>

<span class="token comment"># Override the default request headers:</span>
<span class="token comment"># DEFAULT_REQUEST_HEADERS = &#123;</span>
<span class="token comment">#    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",</span>
<span class="token comment">#    "Accept-Language": "en",</span>
<span class="token comment"># &#125;</span>

<span class="token comment"># Enable or disable spider middlewares</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>
<span class="token comment"># SPIDER_MIDDLEWARES = &#123;</span>
<span class="token comment">#    "scrapy_dushuwang.middlewares.ScrapyDushuwangSpiderMiddleware": 543,</span>
<span class="token comment"># &#125;</span>

<span class="token comment"># Enable or disable downloader middlewares</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span class="token comment"># DOWNLOADER_MIDDLEWARES = &#123;</span>
<span class="token comment">#    "scrapy_dushuwang.middlewares.ScrapyDushuwangDownloaderMiddleware": 543,</span>
<span class="token comment"># &#125;</span>

<span class="token comment"># Enable or disable extensions</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span>
<span class="token comment"># EXTENSIONS = &#123;</span>
<span class="token comment">#    "scrapy.extensions.telnet.TelnetConsole": None,</span>
<span class="token comment"># &#125;</span>

<span class="token comment"># Configure item pipelines</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"scrapy_dushuwang.pipelines.ScrapyDushuwangPipeline"</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
    <span class="token comment"># MySqlPipline:"</span>
    <span class="token string">"scrapy_dushuwang.pipelines.MySqlPipline"</span><span class="token punctuation">:</span> <span class="token number">301</span>
<span class="token punctuation">&#125;</span>

<span class="token comment"># Enable and configure the AutoThrottle extension (disabled by default)</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span>
<span class="token comment"># AUTOTHROTTLE_ENABLED = True</span>
<span class="token comment"># The initial download delay</span>
<span class="token comment"># AUTOTHROTTLE_START_DELAY = 5</span>
<span class="token comment"># The maximum download delay to be set in case of high latencies</span>
<span class="token comment"># AUTOTHROTTLE_MAX_DELAY = 60</span>
<span class="token comment"># The average number of requests Scrapy should be sending in parallel to</span>
<span class="token comment"># each remote server</span>
<span class="token comment"># AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span>
<span class="token comment"># Enable showing throttling stats for every response received:</span>
<span class="token comment"># AUTOTHROTTLE_DEBUG = False</span>

<span class="token comment"># Enable and configure HTTP caching (disabled by default)</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span>
<span class="token comment"># HTTPCACHE_ENABLED = True</span>
<span class="token comment"># HTTPCACHE_EXPIRATION_SECS = 0</span>
<span class="token comment"># HTTPCACHE_DIR = "httpcache"</span>
<span class="token comment"># HTTPCACHE_IGNORE_HTTP_CODES = []</span>
<span class="token comment"># HTTPCACHE_STORAGE = "scrapy.extensions.httpcache.FilesystemCacheStorage"</span>

<span class="token comment"># Set settings whose default value is deprecated to a future-proof value</span>
REQUEST_FINGERPRINTER_IMPLEMENTATION <span class="token operator">=</span> <span class="token string">"2.7"</span>
TWISTED_REACTOR <span class="token operator">=</span> <span class="token string">"twisted.internet.asyncioreactor.AsyncioSelectorReactor"</span>
FEED_EXPORT_ENCODING <span class="token operator">=</span> <span class="token string">"utf-8"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="日志信息和日志等级"><a href="#日志信息和日志等级" class="headerlink" title="日志信息和日志等级"></a>日志信息和日志等级</h2><ul>
<li><p>日志级别:</p>
<ul>
<li><p><strong>CRITICAL</strong>: 严重错误</p>
</li>
<li><p><strong>ERROR</strong>: 一般错误</p>
</li>
<li><p><strong>WARNING</strong>: 警告</p>
</li>
<li><p><strong>INFO</strong>: 一般信息</p>
</li>
<li><p><strong>DEBUG</strong>: 调试信息</p>
</li>
</ul>
<p>  默认的日志等级是DEBUG,只要出现了DEBUG或者DEBUG以上等级的日志,那么这些日志将会打印</p>
</li>
<li><p><code>settings.py</code>文件设置:</p>
<p>  默认的级别为 DEBUG，会显示上面所有的信息</p>
<p>  在配置文件中settings.py</p>
<p>  LOG FILE: 将屏幕显示的信息全部记录到文件中，屏幕不再显示，注意文件后缀一定是<code>.log</code></p>
<p>  LOG LEVEL: 设置日志显示的等级，就是显示哪些，不显示哪些</p>
</li>
</ul>
<h2 id="scrapy的post请求"><a href="#scrapy的post请求" class="headerlink" title="scrapy的post请求"></a>scrapy的post请求</h2><ol>
<li>scrapy中post请求是通过<code>spider/你定义的爬虫程序名.py</code>的<code>start_request()</code>方法实现的</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># post请求 如果没有参数 那么这个请求将没有任何意义</span>
<span class="token comment"># 所以start urls 也没有用了</span>
<span class="token comment"># parse方法也没有用了</span>
<span class="token comment"># start_urls = ['https://fanyi.baidu.com/sug/']</span>
<span class="token comment"># def parse(self, response):</span>
<span class="token comment"># pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="2">
<li>重写start_requests方法:</li>
</ol>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    url <span class="token operator">=</span>'https<span class="token punctuation">:</span><span class="token operator">//</span>fanyi<span class="token punctuation">.</span>baidu<span class="token punctuation">.</span>com<span class="token operator">/</span>sug
    data<span class="token operator">=</span><span class="token punctuation">&#123;</span>
        <span class="token string">'kw'</span><span class="token punctuation">:</span><span class="token string">'final'</span><span class="token punctuation">&#125;</span>
    <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>FormRequest<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span>formdata<span class="token operator">=</span>data<span class="token punctuation">,</span>callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_second<span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">parse_second</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="3">
<li>start requests的返回值:</li>
</ol>
<p><code>scrapy.FormRequest(url=url.headers=headers, callback=self.parse_second, formdata=data)</code></p>
<ul>
<li>url: 要发送的post地址</li>
<li>headers: 可以定制头信息</li>
<li>callback: 回调函数</li>
<li>formdata: post所携带的数据，这是一个字典</li>
</ul>
<h2 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h2><p>到<strong>settings.py</strong>中，打开一个选项<code>DOWNLOADER MIDDLEWARES=&#123;postproject.middlewares.Proxy&#39;:543,&#125;</code></p>
<p>到<strong>middlewares.py</strong>中写代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">process_request</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
	request<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'proxy'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'https://113.68.202.10:9999'</span>
	<span class="token keyword">return</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>



</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://0zxm.github.io">0zxm</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://0zxm.github.io/2024/08/04/scrapy-kuang-jia/">https://0zxm.github.io/2024/08/04/scrapy-kuang-jia/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://0zxm.github.io" target="_blank">0zxm</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/scrapy/">scrapy</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/">数据持久化存储</a></div><div class="post-share"><div class="social-share" data-image="/img/cover1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/08/13/pyinstaller-shi-yong/" title="pyinstaller使用"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">pyinstaller使用</div></div><div class="info-2"><div class="info-item-1">pyinstaller使用入门</div></div></div></a><a class="pagination-related" href="/2024/07/19/javascript-ji-chu-he-pythonweb-kai-fa/" title="JavaScript基础和pythonWeb开发"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default_cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">JavaScript基础和pythonWeb开发</div></div><div class="info-2"><div class="info-item-1">javascript笔记</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/15/django-he-vue3-qian-hou-duan-fen-chi-kai-fa/" title="Django和Vue3前后端补充知识点"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover1.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-15</div><div class="info-item-2">Django和Vue3前后端补充知识点</div></div><div class="info-2"><div class="info-item-1">Django和Vue3前后端分离开发</div></div></div></a><a class="pagination-related" href="/2024/11/23/django-xue-xi/" title="Django学习"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-23</div><div class="info-item-2">Django学习</div></div><div class="info-2"><div class="info-item-1">Django框架的学习</div></div></div></a><a class="pagination-related" href="/2025/01/05/mindspore-shou-xie-shu-zi-shi-bie/" title="Mindspore手写数字识别"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-05</div><div class="info-item-2">Mindspore手写数字识别</div></div><div class="info-2"><div class="info-item-1">使用MindSpore深度学习框架，进行网络搭建、数据处理、网络训练和测试，完成MNIST手写体识别任务</div></div></div></a><a class="pagination-related" href="/2025/02/15/pyqt-xie-yi-ge-dai-ban-cheng-xu/" title="pyqt写一个待办程序"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-15</div><div class="info-item-2">pyqt写一个待办程序</div></div><div class="info-2"><div class="info-item-1">构建一个功能丰富的待办应用：深入PyQt5</div></div></div></a><a class="pagination-related" href="/2024/09/09/pyqt-kai-fa-yi-ge-ji-shi-ben/" title="pyqt开发一个记事本"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover1.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-09</div><div class="info-item-2">pyqt开发一个记事本</div></div><div class="info-2"><div class="info-item-1">构建一个功能丰富的记事本应用：深入 PyQt5</div></div></div></a><a class="pagination-related" href="/2024/11/21/python-zhong-de-import/" title="python中的import"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default_cover.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-21</div><div class="info-item-2">python中的import</div></div><div class="info-2"><div class="info-item-1">介绍python中的import机制及其使用方法</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">0zxm</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">65</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/0zxm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/0zxm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:m15813109801@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="http://0zxm.github.io" target="_blank" title="博客"><i class="fab fa-algolia" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B"><span class="toc-text">Scrapy框架简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E6%A1%86%E6%9E%B6"><span class="toc-text">scrapy框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">scrapy是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85scrapy"><span class="toc-text">安装scrapy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E6%9E%B6%E6%9E%84%E5%9B%BE-%E7%BB%BF%E7%BA%BF%E6%98%AF%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91"><span class="toc-text">Scrapy架构图(绿线是数据流向)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E6%9E%B6%E6%9E%84%E6%B5%81%E7%A8%8B"><span class="toc-text">scrapy架构流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E9%A1%B9%E7%9B%AE%E7%9A%84%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-text">Scrapy项目的运行流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%B6%E4%BD%9CScrapy%E7%88%AC%E8%99%AB%E4%B8%80%E5%85%B1%E5%9B%9B%E6%AD%A5"><span class="toc-text">制作Scrapy爬虫一共四步</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%88%9B%E5%BB%BAscrapy%E9%A1%B9%E7%9B%AE%EF%BC%9A"><span class="toc-text">1.创建scrapy项目：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%A1%B9%E7%9B%AE%E7%BB%84%E6%88%90%EF%BC%9A"><span class="toc-text">2.项目组成：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E5%BB%BA%E7%88%AC%E8%99%AB%E6%96%87%E4%BB%B6%EF%BC%9A"><span class="toc-text">3.创建爬虫文件：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%BF%90%E8%A1%8C%E7%88%AC%E8%99%AB%E6%96%87%E4%BB%B6"><span class="toc-text">4.运行爬虫文件:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E7%BB%88%E7%AB%AF%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4"><span class="toc-text">Scrapy框架终端基本命令</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B"><span class="toc-text">入门案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="toc-text">学习目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%96%B0%E5%BB%BA%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE"><span class="toc-text">1.新建爬虫项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%96%B0%E5%BB%BA%E7%88%AC%E8%99%AB"><span class="toc-text">2.新建爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%BF%90%E8%A1%8C%E5%92%8C%E6%A3%80%E6%B5%8B%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F"><span class="toc-text">3.运行和检测爬虫程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%98%8E%E7%A1%AE%E7%9B%AE%E6%A0%87-Itcast-items-py"><span class="toc-text">4.明确目标(Itcast&#x2F;items.py)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%88%B6%E4%BD%9C%E7%88%AC%E8%99%AB"><span class="toc-text">5.制作爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%88%AC%E6%95%B0%E6%8D%AE-Itcast-spiders-itcast-py"><span class="toc-text">爬数据(Itcast&#x2F;spiders&#x2F;itcast.py)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE"><span class="toc-text">保存数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%9D%E8%80%83"><span class="toc-text">思考</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%A1%E9%81%93%E5%A4%84%E7%90%86%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE-piplines-py"><span class="toc-text">管道处理保存数据(piplines.py)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Item-Pipline"><span class="toc-text">Item Pipline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy-Shell"><span class="toc-text">Scrapy Shell</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8scrapyshell"><span class="toc-text">启动scrapyshell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Selectors%E9%80%89%E6%8B%A9%E5%99%A8"><span class="toc-text">Selectors选择器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spider"><span class="toc-text">Spider</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%8F%82%E8%80%83"><span class="toc-text">源码参考</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E8%85%BE%E8%AE%AF%E6%8B%9B%E8%81%98%E6%A1%88%E4%BE%8B-%E6%99%AE%E9%80%9A%E7%89%88"><span class="toc-text">尝试腾讯招聘案例(普通版)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A6%96%E5%85%88%E5%88%86%E6%9E%90%E8%A6%81%E7%88%AC%E5%8F%96%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-text">首先分析要爬取的数据有哪些</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%84%B6%E5%90%8E%E5%88%9B%E5%BB%BA%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%92%8C%E7%A8%8B%E5%BA%8F"><span class="toc-text">然后创建爬虫项目和程序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#items-py"><span class="toc-text">items.py</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tencent-py"><span class="toc-text">tencent.py</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#piplines-py"><span class="toc-text">piplines.py</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%85%BE%E8%AE%AF%E6%8B%9B%E8%81%98-%E6%8A%93%E5%8C%85"><span class="toc-text">腾讯招聘(抓包)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8A%93%E5%8C%85"><span class="toc-text">什么是抓包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E6%9E%90%E8%AF%B7%E6%B1%82"><span class="toc-text">分析请求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">爬取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-text">编写代码</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#tencent-py-1"><span class="toc-text">tencent.py</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#items-py-1"><span class="toc-text">items.py</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#piplines-py-1"><span class="toc-text">piplines.py</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="toc-text">运行结果展示</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CrawlSpider"><span class="toc-text">CrawlSpider</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%AC%E9%97%A8%E7%A7%98%E7%AC%88"><span class="toc-text">独门秘笈</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CrawlSpider%E6%A1%88%E4%BE%8B"><span class="toc-text">CrawlSpider案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#read-py"><span class="toc-text">read.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#items-py-2"><span class="toc-text">items.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#piplines-py-2"><span class="toc-text">piplines.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#settings-py"><span class="toc-text">settings.py</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E4%BF%A1%E6%81%AF%E5%92%8C%E6%97%A5%E5%BF%97%E7%AD%89%E7%BA%A7"><span class="toc-text">日志信息和日志等级</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scrapy%E7%9A%84post%E8%AF%B7%E6%B1%82"><span class="toc-text">scrapy的post请求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%90%86"><span class="toc-text">代理</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/08/java-ji-chu-xia/" title="Java基础下"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Java基础下"/></a><div class="content"><a class="title" href="/2025/03/08/java-ji-chu-xia/" title="Java基础下">Java基础下</a><time datetime="2025-03-08T03:43:21.000Z" title="发表于 2025-03-08 11:43:21">2025-03-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/07/qian-ru-shi-xi-tong-gai-shu/" title="嵌入式系统概述"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="嵌入式系统概述"/></a><div class="content"><a class="title" href="/2025/03/07/qian-ru-shi-xi-tong-gai-shu/" title="嵌入式系统概述">嵌入式系统概述</a><time datetime="2025-03-07T06:08:48.000Z" title="发表于 2025-03-07 14:08:48">2025-03-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/05/da-shu-ju-ke-shi-hua/" title="大数据可视化"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大数据可视化"/></a><div class="content"><a class="title" href="/2025/03/05/da-shu-ju-ke-shi-hua/" title="大数据可视化">大数据可视化</a><time datetime="2025-03-05T04:51:12.000Z" title="发表于 2025-03-05 12:51:12">2025-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/05/shou-ji-ruan-jian-kai-fa/" title="手机软件开发"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="手机软件开发"/></a><div class="content"><a class="title" href="/2025/03/05/shou-ji-ruan-jian-kai-fa/" title="手机软件开发">手机软件开发</a><time datetime="2025-03-05T04:50:56.000Z" title="发表于 2025-03-05 12:50:56">2025-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/04/ji-suan-ji-tu-xing-xue-he-xu-ni-xian-shi/" title="计算机图形学和虚拟现实"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机图形学和虚拟现实"/></a><div class="content"><a class="title" href="/2025/03/04/ji-suan-ji-tu-xing-xue-he-xu-ni-xian-shi/" title="计算机图形学和虚拟现实">计算机图形学和虚拟现实</a><time datetime="2025-03-04T15:30:27.000Z" title="发表于 2025-03-04 23:30:27">2025-03-04</time></div></div></div></div></div></div><!-- 登录验证模态框--><div id="login-modal"><div class="modal-overlay"></div><div class="modal-content"><h2>Welcome</h2><p>Please enter the password to access this page.</p><input id="password-input" type="password" placeholder="Enter Password"><button id="submit-btn">Submit</button><p id="error-msg" style="color: red; display: none;">Invalid Password. Please try again.</p></div></div></main><footer id="footer" style="background-image: url(/img/footer.jpg);"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By 0zxm</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- CSS 样式--><style>#login-modal {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 1000;
}

.modal-overlay {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background-color: rgba(0, 0, 0, 0.8);
  cursor: pointer;
}

.modal-content {
  background-color: white;
  padding: 20px;
  border-radius: 10px;
  text-align: center;
  width: 80%;
  max-width: 400px;
  box-shadow: 0 0 10px rgba(0,0,0,0.3);
  z-index:999;
}

input#password-input {
  width: 100%;
  padding: 10px;
  margin-top: 10px;
  margin-bottom: 20px;
  border: 1px solid #ccc;
  border-radius: 4px;
}

button#submit-btn {
  background-color: #4CAF50;
  color: white;
  padding: 10px 20px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

button#submit-btn:hover {
  background-color: #45a049;
}
</style><!-- JavaScript 验证--><script>document.addEventListener('DOMContentLoaded', function() {

  (function() {
  // 禁用右键菜单
  document.addEventListener('contextmenu', function(e) {
    e.preventDefault();
    return false;
  });

  // 禁用快捷键 (F12/Ctrl+Shift+I/Ctrl+Shift+J/Ctrl+Shift+C)
  document.addEventListener('keydown', function(e) {
    if (e.key === 'F12' || 
        (e.ctrlKey && e.shiftKey && e.key === 'I') || 
        (e.ctrlKey && e.shiftKey && e.key === 'J') ||
        (e.ctrlKey && e.shiftKey && e.key === 'C')) {
      e.preventDefault();
      return false;
    }
  });
  })();
  

  const modal = document.getElementById('login-modal');
  const passwordInput = document.getElementById('password-input');
  const submitBtn = document.getElementById('submit-btn');
  const errorMsg = document.getElementById('error-msg');
  const bodyWrap = document.getElementById('body-wrap');


  // 默认显示模态框
  modal.style.display = 'flex';

  // 自动聚焦到密码输入框
  passwordInput.focus();

  submitBtn.addEventListener('click', function(e) {
    e.preventDefault();
    const password = passwordInput.value;
    const correctPassword = 'D&X'; // 替换为实际密码

    if (password === correctPassword) {
      modal.style.display = 'none';
      bodyWrap.style.opacity = 1; // 显示页面内容
    } else {
      passwordInput.classList.add('error');
      errorMsg.style.display = 'block';
      setTimeout(() => {
        passwordInput.classList.remove('error');
        errorMsg.style.display = 'none';
      }, 2000);
    }
  });
  // 按下 Enter 键提交
  passwordInput.addEventListener('keypress', function(e) {
    if (e.key === 'Enter') {
      submitBtn.click();
    }
  });
});</script></body></html>